{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for MNIST Digit Classification\n",
    "\n",
    "-  Build a logistic regression model for MNIST\n",
    "- Evaluate model performance using multiple metrics\n",
    "- Fine-tune hyperparameters using grid search\n",
    "- Visualize decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)\n",
    "\n",
    "input_dim = 28 * 28\n",
    "num_classes = 10\n",
    "model = LogisticRegression(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/938], Loss: 0.5689\n",
      "Epoch [1/50], Step [200/938], Loss: 0.3929\n",
      "Epoch [1/50], Step [300/938], Loss: 0.3428\n",
      "Epoch [1/50], Step [400/938], Loss: 0.3514\n",
      "Epoch [1/50], Step [500/938], Loss: 0.3238\n",
      "Epoch [1/50], Step [600/938], Loss: 0.3588\n",
      "Epoch [1/50], Step [700/938], Loss: 0.3198\n",
      "Epoch [1/50], Step [800/938], Loss: 0.3152\n",
      "Epoch [1/50], Step [900/938], Loss: 0.3180\n",
      "Epoch [2/50], Step [100/938], Loss: 0.3007\n",
      "Epoch [2/50], Step [200/938], Loss: 0.3152\n",
      "Epoch [2/50], Step [300/938], Loss: 0.3206\n",
      "Epoch [2/50], Step [400/938], Loss: 0.2941\n",
      "Epoch [2/50], Step [500/938], Loss: 0.3050\n",
      "Epoch [2/50], Step [600/938], Loss: 0.3036\n",
      "Epoch [2/50], Step [700/938], Loss: 0.3020\n",
      "Epoch [2/50], Step [800/938], Loss: 0.3051\n",
      "Epoch [2/50], Step [900/938], Loss: 0.3290\n",
      "Epoch [3/50], Step [100/938], Loss: 0.2941\n",
      "Epoch [3/50], Step [200/938], Loss: 0.2747\n",
      "Epoch [3/50], Step [300/938], Loss: 0.3203\n",
      "Epoch [3/50], Step [400/938], Loss: 0.3103\n",
      "Epoch [3/50], Step [500/938], Loss: 0.3147\n",
      "Epoch [3/50], Step [600/938], Loss: 0.2919\n",
      "Epoch [3/50], Step [700/938], Loss: 0.2595\n",
      "Epoch [3/50], Step [800/938], Loss: 0.3027\n",
      "Epoch [3/50], Step [900/938], Loss: 0.3042\n",
      "Epoch [4/50], Step [100/938], Loss: 0.3021\n",
      "Epoch [4/50], Step [200/938], Loss: 0.2805\n",
      "Epoch [4/50], Step [300/938], Loss: 0.3006\n",
      "Epoch [4/50], Step [400/938], Loss: 0.2899\n",
      "Epoch [4/50], Step [500/938], Loss: 0.2701\n",
      "Epoch [4/50], Step [600/938], Loss: 0.2725\n",
      "Epoch [4/50], Step [700/938], Loss: 0.3026\n",
      "Epoch [4/50], Step [800/938], Loss: 0.3051\n",
      "Epoch [4/50], Step [900/938], Loss: 0.2827\n",
      "Epoch [5/50], Step [100/938], Loss: 0.2817\n",
      "Epoch [5/50], Step [200/938], Loss: 0.2757\n",
      "Epoch [5/50], Step [300/938], Loss: 0.2972\n",
      "Epoch [5/50], Step [400/938], Loss: 0.2975\n",
      "Epoch [5/50], Step [500/938], Loss: 0.2882\n",
      "Epoch [5/50], Step [600/938], Loss: 0.2980\n",
      "Epoch [5/50], Step [700/938], Loss: 0.2777\n",
      "Epoch [5/50], Step [800/938], Loss: 0.2810\n",
      "Epoch [5/50], Step [900/938], Loss: 0.2856\n",
      "Epoch [6/50], Step [100/938], Loss: 0.2901\n",
      "Epoch [6/50], Step [200/938], Loss: 0.2858\n",
      "Epoch [6/50], Step [300/938], Loss: 0.2817\n",
      "Epoch [6/50], Step [400/938], Loss: 0.2830\n",
      "Epoch [6/50], Step [500/938], Loss: 0.2867\n",
      "Epoch [6/50], Step [600/938], Loss: 0.2645\n",
      "Epoch [6/50], Step [700/938], Loss: 0.2921\n",
      "Epoch [6/50], Step [800/938], Loss: 0.2985\n",
      "Epoch [6/50], Step [900/938], Loss: 0.2922\n",
      "Epoch [7/50], Step [100/938], Loss: 0.2588\n",
      "Epoch [7/50], Step [200/938], Loss: 0.2794\n",
      "Epoch [7/50], Step [300/938], Loss: 0.2770\n",
      "Epoch [7/50], Step [400/938], Loss: 0.2956\n",
      "Epoch [7/50], Step [500/938], Loss: 0.3041\n",
      "Epoch [7/50], Step [600/938], Loss: 0.2796\n",
      "Epoch [7/50], Step [700/938], Loss: 0.2736\n",
      "Epoch [7/50], Step [800/938], Loss: 0.2939\n",
      "Epoch [7/50], Step [900/938], Loss: 0.2769\n",
      "Epoch [8/50], Step [100/938], Loss: 0.2594\n",
      "Epoch [8/50], Step [200/938], Loss: 0.2874\n",
      "Epoch [8/50], Step [300/938], Loss: 0.2862\n",
      "Epoch [8/50], Step [400/938], Loss: 0.2729\n",
      "Epoch [8/50], Step [500/938], Loss: 0.2705\n",
      "Epoch [8/50], Step [600/938], Loss: 0.2988\n",
      "Epoch [8/50], Step [700/938], Loss: 0.2854\n",
      "Epoch [8/50], Step [800/938], Loss: 0.2999\n",
      "Epoch [8/50], Step [900/938], Loss: 0.2785\n",
      "Epoch [9/50], Step [100/938], Loss: 0.2721\n",
      "Epoch [9/50], Step [200/938], Loss: 0.2750\n",
      "Epoch [9/50], Step [300/938], Loss: 0.2894\n",
      "Epoch [9/50], Step [400/938], Loss: 0.2793\n",
      "Epoch [9/50], Step [500/938], Loss: 0.2703\n",
      "Epoch [9/50], Step [600/938], Loss: 0.2724\n",
      "Epoch [9/50], Step [700/938], Loss: 0.2873\n",
      "Epoch [9/50], Step [800/938], Loss: 0.2769\n",
      "Epoch [9/50], Step [900/938], Loss: 0.2717\n",
      "Epoch [10/50], Step [100/938], Loss: 0.2653\n",
      "Epoch [10/50], Step [200/938], Loss: 0.2733\n",
      "Epoch [10/50], Step [300/938], Loss: 0.2771\n",
      "Epoch [10/50], Step [400/938], Loss: 0.2660\n",
      "Epoch [10/50], Step [500/938], Loss: 0.2785\n",
      "Epoch [10/50], Step [600/938], Loss: 0.2817\n",
      "Epoch [10/50], Step [700/938], Loss: 0.2761\n",
      "Epoch [10/50], Step [800/938], Loss: 0.2917\n",
      "Epoch [10/50], Step [900/938], Loss: 0.2799\n",
      "Epoch [11/50], Step [100/938], Loss: 0.2522\n",
      "Epoch [11/50], Step [200/938], Loss: 0.2721\n",
      "Epoch [11/50], Step [300/938], Loss: 0.2513\n",
      "Epoch [11/50], Step [400/938], Loss: 0.2648\n",
      "Epoch [11/50], Step [500/938], Loss: 0.2752\n",
      "Epoch [11/50], Step [600/938], Loss: 0.2980\n",
      "Epoch [11/50], Step [700/938], Loss: 0.2735\n",
      "Epoch [11/50], Step [800/938], Loss: 0.2887\n",
      "Epoch [11/50], Step [900/938], Loss: 0.3013\n",
      "Epoch [12/50], Step [100/938], Loss: 0.2706\n",
      "Epoch [12/50], Step [200/938], Loss: 0.2663\n",
      "Epoch [12/50], Step [300/938], Loss: 0.2886\n",
      "Epoch [12/50], Step [400/938], Loss: 0.2702\n",
      "Epoch [12/50], Step [500/938], Loss: 0.2617\n",
      "Epoch [12/50], Step [600/938], Loss: 0.2899\n",
      "Epoch [12/50], Step [700/938], Loss: 0.2790\n",
      "Epoch [12/50], Step [800/938], Loss: 0.2611\n",
      "Epoch [12/50], Step [900/938], Loss: 0.2850\n",
      "Epoch [13/50], Step [100/938], Loss: 0.2739\n",
      "Epoch [13/50], Step [200/938], Loss: 0.2718\n",
      "Epoch [13/50], Step [300/938], Loss: 0.2724\n",
      "Epoch [13/50], Step [400/938], Loss: 0.2685\n",
      "Epoch [13/50], Step [500/938], Loss: 0.3109\n",
      "Epoch [13/50], Step [600/938], Loss: 0.2516\n",
      "Epoch [13/50], Step [700/938], Loss: 0.2708\n",
      "Epoch [13/50], Step [800/938], Loss: 0.2663\n",
      "Epoch [13/50], Step [900/938], Loss: 0.2629\n",
      "Epoch [14/50], Step [100/938], Loss: 0.2681\n",
      "Epoch [14/50], Step [200/938], Loss: 0.2711\n",
      "Epoch [14/50], Step [300/938], Loss: 0.2605\n",
      "Epoch [14/50], Step [400/938], Loss: 0.2845\n",
      "Epoch [14/50], Step [500/938], Loss: 0.2838\n",
      "Epoch [14/50], Step [600/938], Loss: 0.2824\n",
      "Epoch [14/50], Step [700/938], Loss: 0.2804\n",
      "Epoch [14/50], Step [800/938], Loss: 0.2625\n",
      "Epoch [14/50], Step [900/938], Loss: 0.2682\n",
      "Epoch [15/50], Step [100/938], Loss: 0.2638\n",
      "Epoch [15/50], Step [200/938], Loss: 0.2591\n",
      "Epoch [15/50], Step [300/938], Loss: 0.2742\n",
      "Epoch [15/50], Step [400/938], Loss: 0.2724\n",
      "Epoch [15/50], Step [500/938], Loss: 0.2784\n",
      "Epoch [15/50], Step [600/938], Loss: 0.2696\n",
      "Epoch [15/50], Step [700/938], Loss: 0.2791\n",
      "Epoch [15/50], Step [800/938], Loss: 0.2804\n",
      "Epoch [15/50], Step [900/938], Loss: 0.2571\n",
      "Epoch [16/50], Step [100/938], Loss: 0.2479\n",
      "Epoch [16/50], Step [200/938], Loss: 0.2437\n",
      "Epoch [16/50], Step [300/938], Loss: 0.2662\n",
      "Epoch [16/50], Step [400/938], Loss: 0.2678\n",
      "Epoch [16/50], Step [500/938], Loss: 0.2778\n",
      "Epoch [16/50], Step [600/938], Loss: 0.2761\n",
      "Epoch [16/50], Step [700/938], Loss: 0.2894\n",
      "Epoch [16/50], Step [800/938], Loss: 0.2760\n",
      "Epoch [16/50], Step [900/938], Loss: 0.2571\n",
      "Epoch [17/50], Step [100/938], Loss: 0.2727\n",
      "Epoch [17/50], Step [200/938], Loss: 0.2384\n",
      "Epoch [17/50], Step [300/938], Loss: 0.2789\n",
      "Epoch [17/50], Step [400/938], Loss: 0.2625\n",
      "Epoch [17/50], Step [500/938], Loss: 0.2640\n",
      "Epoch [17/50], Step [600/938], Loss: 0.2773\n",
      "Epoch [17/50], Step [700/938], Loss: 0.2557\n",
      "Epoch [17/50], Step [800/938], Loss: 0.2822\n",
      "Epoch [17/50], Step [900/938], Loss: 0.2603\n",
      "Epoch [18/50], Step [100/938], Loss: 0.2422\n",
      "Epoch [18/50], Step [200/938], Loss: 0.2858\n",
      "Epoch [18/50], Step [300/938], Loss: 0.2532\n",
      "Epoch [18/50], Step [400/938], Loss: 0.2574\n",
      "Epoch [18/50], Step [500/938], Loss: 0.2659\n",
      "Epoch [18/50], Step [600/938], Loss: 0.2904\n",
      "Epoch [18/50], Step [700/938], Loss: 0.2755\n",
      "Epoch [18/50], Step [800/938], Loss: 0.2826\n",
      "Epoch [18/50], Step [900/938], Loss: 0.2841\n",
      "Epoch [19/50], Step [100/938], Loss: 0.2775\n",
      "Epoch [19/50], Step [200/938], Loss: 0.2500\n",
      "Epoch [19/50], Step [300/938], Loss: 0.2676\n",
      "Epoch [19/50], Step [400/938], Loss: 0.2812\n",
      "Epoch [19/50], Step [500/938], Loss: 0.2706\n",
      "Epoch [19/50], Step [600/938], Loss: 0.2627\n",
      "Epoch [19/50], Step [700/938], Loss: 0.2462\n",
      "Epoch [19/50], Step [800/938], Loss: 0.2739\n",
      "Epoch [19/50], Step [900/938], Loss: 0.2709\n",
      "Epoch [20/50], Step [100/938], Loss: 0.2400\n",
      "Epoch [20/50], Step [200/938], Loss: 0.2436\n",
      "Epoch [20/50], Step [300/938], Loss: 0.2613\n",
      "Epoch [20/50], Step [400/938], Loss: 0.2673\n",
      "Epoch [20/50], Step [500/938], Loss: 0.2736\n",
      "Epoch [20/50], Step [600/938], Loss: 0.2735\n",
      "Epoch [20/50], Step [700/938], Loss: 0.2808\n",
      "Epoch [20/50], Step [800/938], Loss: 0.2823\n",
      "Epoch [20/50], Step [900/938], Loss: 0.2714\n",
      "Epoch [21/50], Step [100/938], Loss: 0.2596\n",
      "Epoch [21/50], Step [200/938], Loss: 0.2679\n",
      "Epoch [21/50], Step [300/938], Loss: 0.2571\n",
      "Epoch [21/50], Step [400/938], Loss: 0.2766\n",
      "Epoch [21/50], Step [500/938], Loss: 0.2940\n",
      "Epoch [21/50], Step [600/938], Loss: 0.2431\n",
      "Epoch [21/50], Step [700/938], Loss: 0.2695\n",
      "Epoch [21/50], Step [800/938], Loss: 0.2579\n",
      "Epoch [21/50], Step [900/938], Loss: 0.2756\n",
      "Epoch [22/50], Step [100/938], Loss: 0.2520\n",
      "Epoch [22/50], Step [200/938], Loss: 0.2340\n",
      "Epoch [22/50], Step [300/938], Loss: 0.2456\n",
      "Epoch [22/50], Step [400/938], Loss: 0.2791\n",
      "Epoch [22/50], Step [500/938], Loss: 0.2729\n",
      "Epoch [22/50], Step [600/938], Loss: 0.2570\n",
      "Epoch [22/50], Step [700/938], Loss: 0.2892\n",
      "Epoch [22/50], Step [800/938], Loss: 0.2879\n",
      "Epoch [22/50], Step [900/938], Loss: 0.2807\n",
      "Epoch [23/50], Step [100/938], Loss: 0.2650\n",
      "Epoch [23/50], Step [200/938], Loss: 0.2860\n",
      "Epoch [23/50], Step [300/938], Loss: 0.2644\n",
      "Epoch [23/50], Step [400/938], Loss: 0.2897\n",
      "Epoch [23/50], Step [500/938], Loss: 0.2602\n",
      "Epoch [23/50], Step [600/938], Loss: 0.2731\n",
      "Epoch [23/50], Step [700/938], Loss: 0.2547\n",
      "Epoch [23/50], Step [800/938], Loss: 0.2483\n",
      "Epoch [23/50], Step [900/938], Loss: 0.2502\n",
      "Epoch [24/50], Step [100/938], Loss: 0.2630\n",
      "Epoch [24/50], Step [200/938], Loss: 0.2639\n",
      "Epoch [24/50], Step [300/938], Loss: 0.2904\n",
      "Epoch [24/50], Step [400/938], Loss: 0.2520\n",
      "Epoch [24/50], Step [500/938], Loss: 0.2560\n",
      "Epoch [24/50], Step [600/938], Loss: 0.2598\n",
      "Epoch [24/50], Step [700/938], Loss: 0.2473\n",
      "Epoch [24/50], Step [800/938], Loss: 0.2694\n",
      "Epoch [24/50], Step [900/938], Loss: 0.2754\n",
      "Epoch [25/50], Step [100/938], Loss: 0.2587\n",
      "Epoch [25/50], Step [200/938], Loss: 0.2398\n",
      "Epoch [25/50], Step [300/938], Loss: 0.2746\n",
      "Epoch [25/50], Step [400/938], Loss: 0.2567\n",
      "Epoch [25/50], Step [500/938], Loss: 0.2657\n",
      "Epoch [25/50], Step [600/938], Loss: 0.2723\n",
      "Epoch [25/50], Step [700/938], Loss: 0.2830\n",
      "Epoch [25/50], Step [800/938], Loss: 0.2766\n",
      "Epoch [25/50], Step [900/938], Loss: 0.2651\n",
      "Epoch [26/50], Step [100/938], Loss: 0.2302\n",
      "Epoch [26/50], Step [200/938], Loss: 0.2342\n",
      "Epoch [26/50], Step [300/938], Loss: 0.2467\n",
      "Epoch [26/50], Step [400/938], Loss: 0.2595\n",
      "Epoch [26/50], Step [500/938], Loss: 0.2614\n",
      "Epoch [26/50], Step [600/938], Loss: 0.2819\n",
      "Epoch [26/50], Step [700/938], Loss: 0.2735\n",
      "Epoch [26/50], Step [800/938], Loss: 0.2998\n",
      "Epoch [26/50], Step [900/938], Loss: 0.2882\n",
      "Epoch [27/50], Step [100/938], Loss: 0.2400\n",
      "Epoch [27/50], Step [200/938], Loss: 0.2366\n",
      "Epoch [27/50], Step [300/938], Loss: 0.2387\n",
      "Epoch [27/50], Step [400/938], Loss: 0.2777\n",
      "Epoch [27/50], Step [500/938], Loss: 0.2654\n",
      "Epoch [27/50], Step [600/938], Loss: 0.2645\n",
      "Epoch [27/50], Step [700/938], Loss: 0.2695\n",
      "Epoch [27/50], Step [800/938], Loss: 0.2991\n",
      "Epoch [27/50], Step [900/938], Loss: 0.2882\n",
      "Epoch [28/50], Step [100/938], Loss: 0.2529\n",
      "Epoch [28/50], Step [200/938], Loss: 0.2523\n",
      "Epoch [28/50], Step [300/938], Loss: 0.2459\n",
      "Epoch [28/50], Step [400/938], Loss: 0.2505\n",
      "Epoch [28/50], Step [500/938], Loss: 0.2853\n",
      "Epoch [28/50], Step [600/938], Loss: 0.2904\n",
      "Epoch [28/50], Step [700/938], Loss: 0.2509\n",
      "Epoch [28/50], Step [800/938], Loss: 0.2688\n",
      "Epoch [28/50], Step [900/938], Loss: 0.2700\n",
      "Epoch [29/50], Step [100/938], Loss: 0.2469\n",
      "Epoch [29/50], Step [200/938], Loss: 0.2687\n",
      "Epoch [29/50], Step [300/938], Loss: 0.2534\n",
      "Epoch [29/50], Step [400/938], Loss: 0.2801\n",
      "Epoch [29/50], Step [500/938], Loss: 0.2616\n",
      "Epoch [29/50], Step [600/938], Loss: 0.2637\n",
      "Epoch [29/50], Step [700/938], Loss: 0.2571\n",
      "Epoch [29/50], Step [800/938], Loss: 0.2769\n",
      "Epoch [29/50], Step [900/938], Loss: 0.2501\n",
      "Epoch [30/50], Step [100/938], Loss: 0.2526\n",
      "Epoch [30/50], Step [200/938], Loss: 0.2475\n",
      "Epoch [30/50], Step [300/938], Loss: 0.2615\n",
      "Epoch [30/50], Step [400/938], Loss: 0.2754\n",
      "Epoch [30/50], Step [500/938], Loss: 0.2816\n",
      "Epoch [30/50], Step [600/938], Loss: 0.2449\n",
      "Epoch [30/50], Step [700/938], Loss: 0.2531\n",
      "Epoch [30/50], Step [800/938], Loss: 0.2678\n",
      "Epoch [30/50], Step [900/938], Loss: 0.2806\n",
      "Epoch [31/50], Step [100/938], Loss: 0.2580\n",
      "Epoch [31/50], Step [200/938], Loss: 0.2441\n",
      "Epoch [31/50], Step [300/938], Loss: 0.2485\n",
      "Epoch [31/50], Step [400/938], Loss: 0.2659\n",
      "Epoch [31/50], Step [500/938], Loss: 0.2811\n",
      "Epoch [31/50], Step [600/938], Loss: 0.2641\n",
      "Epoch [31/50], Step [700/938], Loss: 0.2858\n",
      "Epoch [31/50], Step [800/938], Loss: 0.2700\n",
      "Epoch [31/50], Step [900/938], Loss: 0.2689\n",
      "Epoch [32/50], Step [100/938], Loss: 0.2343\n",
      "Epoch [32/50], Step [200/938], Loss: 0.2617\n",
      "Epoch [32/50], Step [300/938], Loss: 0.2526\n",
      "Epoch [32/50], Step [400/938], Loss: 0.2634\n",
      "Epoch [32/50], Step [500/938], Loss: 0.3046\n",
      "Epoch [32/50], Step [600/938], Loss: 0.2673\n",
      "Epoch [32/50], Step [700/938], Loss: 0.2635\n",
      "Epoch [32/50], Step [800/938], Loss: 0.2676\n",
      "Epoch [32/50], Step [900/938], Loss: 0.2616\n",
      "Epoch [33/50], Step [100/938], Loss: 0.2689\n",
      "Epoch [33/50], Step [200/938], Loss: 0.2554\n",
      "Epoch [33/50], Step [300/938], Loss: 0.2522\n",
      "Epoch [33/50], Step [400/938], Loss: 0.2420\n",
      "Epoch [33/50], Step [500/938], Loss: 0.2802\n",
      "Epoch [33/50], Step [600/938], Loss: 0.2411\n",
      "Epoch [33/50], Step [700/938], Loss: 0.2790\n",
      "Epoch [33/50], Step [800/938], Loss: 0.2566\n",
      "Epoch [33/50], Step [900/938], Loss: 0.2642\n",
      "Epoch [34/50], Step [100/938], Loss: 0.2493\n",
      "Epoch [34/50], Step [200/938], Loss: 0.2649\n",
      "Epoch [34/50], Step [300/938], Loss: 0.2855\n",
      "Epoch [34/50], Step [400/938], Loss: 0.2516\n",
      "Epoch [34/50], Step [500/938], Loss: 0.2680\n",
      "Epoch [34/50], Step [600/938], Loss: 0.2622\n",
      "Epoch [34/50], Step [700/938], Loss: 0.2596\n",
      "Epoch [34/50], Step [800/938], Loss: 0.2452\n",
      "Epoch [34/50], Step [900/938], Loss: 0.2769\n",
      "Epoch [35/50], Step [100/938], Loss: 0.2357\n",
      "Epoch [35/50], Step [200/938], Loss: 0.2708\n",
      "Epoch [35/50], Step [300/938], Loss: 0.2578\n",
      "Epoch [35/50], Step [400/938], Loss: 0.2524\n",
      "Epoch [35/50], Step [500/938], Loss: 0.2520\n",
      "Epoch [35/50], Step [600/938], Loss: 0.2541\n",
      "Epoch [35/50], Step [700/938], Loss: 0.2782\n",
      "Epoch [35/50], Step [800/938], Loss: 0.2625\n",
      "Epoch [35/50], Step [900/938], Loss: 0.2822\n",
      "Epoch [36/50], Step [100/938], Loss: 0.2447\n",
      "Epoch [36/50], Step [200/938], Loss: 0.2344\n",
      "Epoch [36/50], Step [300/938], Loss: 0.2464\n",
      "Epoch [36/50], Step [400/938], Loss: 0.2685\n",
      "Epoch [36/50], Step [500/938], Loss: 0.2549\n",
      "Epoch [36/50], Step [600/938], Loss: 0.2849\n",
      "Epoch [36/50], Step [700/938], Loss: 0.2539\n",
      "Epoch [36/50], Step [800/938], Loss: 0.2797\n",
      "Epoch [36/50], Step [900/938], Loss: 0.2916\n",
      "Epoch [37/50], Step [100/938], Loss: 0.2562\n",
      "Epoch [37/50], Step [200/938], Loss: 0.2552\n",
      "Epoch [37/50], Step [300/938], Loss: 0.2819\n",
      "Epoch [37/50], Step [400/938], Loss: 0.2593\n",
      "Epoch [37/50], Step [500/938], Loss: 0.2562\n",
      "Epoch [37/50], Step [600/938], Loss: 0.2412\n",
      "Epoch [37/50], Step [700/938], Loss: 0.2878\n",
      "Epoch [37/50], Step [800/938], Loss: 0.2565\n",
      "Epoch [37/50], Step [900/938], Loss: 0.2551\n",
      "Epoch [38/50], Step [100/938], Loss: 0.2479\n",
      "Epoch [38/50], Step [200/938], Loss: 0.2658\n",
      "Epoch [38/50], Step [300/938], Loss: 0.2492\n",
      "Epoch [38/50], Step [400/938], Loss: 0.2503\n",
      "Epoch [38/50], Step [500/938], Loss: 0.2808\n",
      "Epoch [38/50], Step [600/938], Loss: 0.2646\n",
      "Epoch [38/50], Step [700/938], Loss: 0.2466\n",
      "Epoch [38/50], Step [800/938], Loss: 0.2643\n",
      "Epoch [38/50], Step [900/938], Loss: 0.2791\n",
      "Epoch [39/50], Step [100/938], Loss: 0.2509\n",
      "Epoch [39/50], Step [200/938], Loss: 0.2710\n",
      "Epoch [39/50], Step [300/938], Loss: 0.2797\n",
      "Epoch [39/50], Step [400/938], Loss: 0.2492\n",
      "Epoch [39/50], Step [500/938], Loss: 0.2403\n",
      "Epoch [39/50], Step [600/938], Loss: 0.2666\n",
      "Epoch [39/50], Step [700/938], Loss: 0.2634\n",
      "Epoch [39/50], Step [800/938], Loss: 0.2567\n",
      "Epoch [39/50], Step [900/938], Loss: 0.2480\n",
      "Epoch [40/50], Step [100/938], Loss: 0.2446\n",
      "Epoch [40/50], Step [200/938], Loss: 0.2492\n",
      "Epoch [40/50], Step [300/938], Loss: 0.2661\n",
      "Epoch [40/50], Step [400/938], Loss: 0.2722\n",
      "Epoch [40/50], Step [500/938], Loss: 0.2533\n",
      "Epoch [40/50], Step [600/938], Loss: 0.2626\n",
      "Epoch [40/50], Step [700/938], Loss: 0.2687\n",
      "Epoch [40/50], Step [800/938], Loss: 0.2644\n",
      "Epoch [40/50], Step [900/938], Loss: 0.2707\n",
      "Epoch [41/50], Step [100/938], Loss: 0.2391\n",
      "Epoch [41/50], Step [200/938], Loss: 0.2613\n",
      "Epoch [41/50], Step [300/938], Loss: 0.2640\n",
      "Epoch [41/50], Step [400/938], Loss: 0.2686\n",
      "Epoch [41/50], Step [500/938], Loss: 0.2719\n",
      "Epoch [41/50], Step [600/938], Loss: 0.2453\n",
      "Epoch [41/50], Step [700/938], Loss: 0.2466\n",
      "Epoch [41/50], Step [800/938], Loss: 0.2730\n",
      "Epoch [41/50], Step [900/938], Loss: 0.2744\n",
      "Epoch [42/50], Step [100/938], Loss: 0.2738\n",
      "Epoch [42/50], Step [200/938], Loss: 0.2297\n",
      "Epoch [42/50], Step [300/938], Loss: 0.2452\n",
      "Epoch [42/50], Step [400/938], Loss: 0.2443\n",
      "Epoch [42/50], Step [500/938], Loss: 0.2717\n",
      "Epoch [42/50], Step [600/938], Loss: 0.2739\n",
      "Epoch [42/50], Step [700/938], Loss: 0.2705\n",
      "Epoch [42/50], Step [800/938], Loss: 0.2814\n",
      "Epoch [42/50], Step [900/938], Loss: 0.2643\n",
      "Epoch [43/50], Step [100/938], Loss: 0.2636\n",
      "Epoch [43/50], Step [200/938], Loss: 0.2658\n",
      "Epoch [43/50], Step [300/938], Loss: 0.2588\n",
      "Epoch [43/50], Step [400/938], Loss: 0.2616\n",
      "Epoch [43/50], Step [500/938], Loss: 0.2535\n",
      "Epoch [43/50], Step [600/938], Loss: 0.2746\n",
      "Epoch [43/50], Step [700/938], Loss: 0.2572\n",
      "Epoch [43/50], Step [800/938], Loss: 0.2562\n",
      "Epoch [43/50], Step [900/938], Loss: 0.2583\n",
      "Epoch [44/50], Step [100/938], Loss: 0.2606\n",
      "Epoch [44/50], Step [200/938], Loss: 0.2567\n",
      "Epoch [44/50], Step [300/938], Loss: 0.2461\n",
      "Epoch [44/50], Step [400/938], Loss: 0.2684\n",
      "Epoch [44/50], Step [500/938], Loss: 0.2643\n",
      "Epoch [44/50], Step [600/938], Loss: 0.2402\n",
      "Epoch [44/50], Step [700/938], Loss: 0.2683\n",
      "Epoch [44/50], Step [800/938], Loss: 0.2806\n",
      "Epoch [44/50], Step [900/938], Loss: 0.2581\n",
      "Epoch [45/50], Step [100/938], Loss: 0.2553\n",
      "Epoch [45/50], Step [200/938], Loss: 0.2632\n",
      "Epoch [45/50], Step [300/938], Loss: 0.2536\n",
      "Epoch [45/50], Step [400/938], Loss: 0.2408\n",
      "Epoch [45/50], Step [500/938], Loss: 0.2516\n",
      "Epoch [45/50], Step [600/938], Loss: 0.2529\n",
      "Epoch [45/50], Step [700/938], Loss: 0.2798\n",
      "Epoch [45/50], Step [800/938], Loss: 0.2592\n",
      "Epoch [45/50], Step [900/938], Loss: 0.2688\n",
      "Epoch [46/50], Step [100/938], Loss: 0.2497\n",
      "Epoch [46/50], Step [200/938], Loss: 0.2361\n",
      "Epoch [46/50], Step [300/938], Loss: 0.2648\n",
      "Epoch [46/50], Step [400/938], Loss: 0.2541\n",
      "Epoch [46/50], Step [500/938], Loss: 0.2724\n",
      "Epoch [46/50], Step [600/938], Loss: 0.2449\n",
      "Epoch [46/50], Step [700/938], Loss: 0.2617\n",
      "Epoch [46/50], Step [800/938], Loss: 0.2888\n",
      "Epoch [46/50], Step [900/938], Loss: 0.2548\n",
      "Epoch [47/50], Step [100/938], Loss: 0.2622\n",
      "Epoch [47/50], Step [200/938], Loss: 0.2569\n",
      "Epoch [47/50], Step [300/938], Loss: 0.2395\n",
      "Epoch [47/50], Step [400/938], Loss: 0.2729\n",
      "Epoch [47/50], Step [500/938], Loss: 0.2647\n",
      "Epoch [47/50], Step [600/938], Loss: 0.2609\n",
      "Epoch [47/50], Step [700/938], Loss: 0.2482\n",
      "Epoch [47/50], Step [800/938], Loss: 0.2714\n",
      "Epoch [47/50], Step [900/938], Loss: 0.2613\n",
      "Epoch [48/50], Step [100/938], Loss: 0.2593\n",
      "Epoch [48/50], Step [200/938], Loss: 0.2440\n",
      "Epoch [48/50], Step [300/938], Loss: 0.2456\n",
      "Epoch [48/50], Step [400/938], Loss: 0.2581\n",
      "Epoch [48/50], Step [500/938], Loss: 0.2541\n",
      "Epoch [48/50], Step [600/938], Loss: 0.2573\n",
      "Epoch [48/50], Step [700/938], Loss: 0.2546\n",
      "Epoch [48/50], Step [800/938], Loss: 0.2762\n",
      "Epoch [48/50], Step [900/938], Loss: 0.2831\n",
      "Epoch [49/50], Step [100/938], Loss: 0.2503\n",
      "Epoch [49/50], Step [200/938], Loss: 0.2203\n",
      "Epoch [49/50], Step [300/938], Loss: 0.2467\n",
      "Epoch [49/50], Step [400/938], Loss: 0.2501\n",
      "Epoch [49/50], Step [500/938], Loss: 0.2793\n",
      "Epoch [49/50], Step [600/938], Loss: 0.2542\n",
      "Epoch [49/50], Step [700/938], Loss: 0.2944\n",
      "Epoch [49/50], Step [800/938], Loss: 0.2654\n",
      "Epoch [49/50], Step [900/938], Loss: 0.2747\n",
      "Epoch [50/50], Step [100/938], Loss: 0.2359\n",
      "Epoch [50/50], Step [200/938], Loss: 0.2611\n",
      "Epoch [50/50], Step [300/938], Loss: 0.2554\n",
      "Epoch [50/50], Step [400/938], Loss: 0.2586\n",
      "Epoch [50/50], Step [500/938], Loss: 0.2841\n",
      "Epoch [50/50], Step [600/938], Loss: 0.2472\n",
      "Epoch [50/50], Step [700/938], Loss: 0.2433\n",
      "Epoch [50/50], Step [800/938], Loss: 0.2931\n",
      "Epoch [50/50], Step [900/938], Loss: 0.2549\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "num_epochs = 50\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9230\n",
      "Precision: 0.9232\n",
      "Recall: 0.9230\n",
      "F1 Score: 0.9229\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate(model, test_loader)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/938], Loss: 1.3033\n",
      "Epoch [1/2], Step [200/938], Loss: 0.6792\n",
      "Epoch [1/2], Step [300/938], Loss: 0.5759\n",
      "Epoch [1/2], Step [400/938], Loss: 0.4931\n",
      "Epoch [1/2], Step [500/938], Loss: 0.4729\n",
      "Epoch [1/2], Step [600/938], Loss: 0.4709\n",
      "Epoch [1/2], Step [700/938], Loss: 0.4284\n",
      "Epoch [1/2], Step [800/938], Loss: 0.4106\n",
      "Epoch [1/2], Step [900/938], Loss: 0.4079\n",
      "Epoch [2/2], Step [100/938], Loss: 0.3912\n",
      "Epoch [2/2], Step [200/938], Loss: 0.3704\n",
      "Epoch [2/2], Step [300/938], Loss: 0.3738\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3892\n",
      "Epoch [2/2], Step [500/938], Loss: 0.3578\n",
      "Epoch [2/2], Step [600/938], Loss: 0.3614\n",
      "Epoch [2/2], Step [700/938], Loss: 0.3547\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3784\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3546\n",
      "LR: 0.001, Momentum: 0.8, Accuracy: 0.9069\n",
      "Epoch [1/2], Step [100/938], Loss: 1.0223\n",
      "Epoch [1/2], Step [200/938], Loss: 0.5210\n",
      "Epoch [1/2], Step [300/938], Loss: 0.4628\n",
      "Epoch [1/2], Step [400/938], Loss: 0.4298\n",
      "Epoch [1/2], Step [500/938], Loss: 0.3941\n",
      "Epoch [1/2], Step [600/938], Loss: 0.3854\n",
      "Epoch [1/2], Step [700/938], Loss: 0.3774\n",
      "Epoch [1/2], Step [800/938], Loss: 0.3695\n",
      "Epoch [1/2], Step [900/938], Loss: 0.3507\n",
      "Epoch [2/2], Step [100/938], Loss: 0.3359\n",
      "Epoch [2/2], Step [200/938], Loss: 0.3258\n",
      "Epoch [2/2], Step [300/938], Loss: 0.3405\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3344\n",
      "Epoch [2/2], Step [500/938], Loss: 0.3352\n",
      "Epoch [2/2], Step [600/938], Loss: 0.3432\n",
      "Epoch [2/2], Step [700/938], Loss: 0.3357\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3053\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3432\n",
      "LR: 0.001, Momentum: 0.9, Accuracy: 0.9155\n",
      "Epoch [1/2], Step [100/938], Loss: 0.8813\n",
      "Epoch [1/2], Step [200/938], Loss: 0.4264\n",
      "Epoch [1/2], Step [300/938], Loss: 0.3869\n",
      "Epoch [1/2], Step [400/938], Loss: 0.3762\n",
      "Epoch [1/2], Step [500/938], Loss: 0.3489\n",
      "Epoch [1/2], Step [600/938], Loss: 0.3363\n",
      "Epoch [1/2], Step [700/938], Loss: 0.3379\n",
      "Epoch [1/2], Step [800/938], Loss: 0.3386\n",
      "Epoch [1/2], Step [900/938], Loss: 0.3357\n",
      "Epoch [2/2], Step [100/938], Loss: 0.3138\n",
      "Epoch [2/2], Step [200/938], Loss: 0.3140\n",
      "Epoch [2/2], Step [300/938], Loss: 0.3252\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3009\n",
      "Epoch [2/2], Step [500/938], Loss: 0.3155\n",
      "Epoch [2/2], Step [600/938], Loss: 0.2957\n",
      "Epoch [2/2], Step [700/938], Loss: 0.3101\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3168\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3003\n",
      "LR: 0.001, Momentum: 0.95, Accuracy: 0.9170\n",
      "Epoch [1/2], Step [100/938], Loss: 0.6216\n",
      "Epoch [1/2], Step [200/938], Loss: 0.3776\n",
      "Epoch [1/2], Step [300/938], Loss: 0.3465\n",
      "Epoch [1/2], Step [400/938], Loss: 0.3283\n",
      "Epoch [1/2], Step [500/938], Loss: 0.3418\n",
      "Epoch [1/2], Step [600/938], Loss: 0.3179\n",
      "Epoch [1/2], Step [700/938], Loss: 0.3292\n",
      "Epoch [1/2], Step [800/938], Loss: 0.3022\n",
      "Epoch [1/2], Step [900/938], Loss: 0.3330\n",
      "Epoch [2/2], Step [100/938], Loss: 0.3122\n",
      "Epoch [2/2], Step [200/938], Loss: 0.3100\n",
      "Epoch [2/2], Step [300/938], Loss: 0.3009\n",
      "Epoch [2/2], Step [400/938], Loss: 0.2966\n",
      "Epoch [2/2], Step [500/938], Loss: 0.2935\n",
      "Epoch [2/2], Step [600/938], Loss: 0.2898\n",
      "Epoch [2/2], Step [700/938], Loss: 0.2713\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3040\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3027\n",
      "LR: 0.01, Momentum: 0.8, Accuracy: 0.9192\n",
      "Epoch [1/2], Step [100/938], Loss: 0.5849\n",
      "Epoch [1/2], Step [200/938], Loss: 0.3594\n",
      "Epoch [1/2], Step [300/938], Loss: 0.3402\n",
      "Epoch [1/2], Step [400/938], Loss: 0.3284\n",
      "Epoch [1/2], Step [500/938], Loss: 0.3264\n",
      "Epoch [1/2], Step [600/938], Loss: 0.3433\n",
      "Epoch [1/2], Step [700/938], Loss: 0.3434\n",
      "Epoch [1/2], Step [800/938], Loss: 0.3102\n",
      "Epoch [1/2], Step [900/938], Loss: 0.3361\n",
      "Epoch [2/2], Step [100/938], Loss: 0.2904\n",
      "Epoch [2/2], Step [200/938], Loss: 0.2980\n",
      "Epoch [2/2], Step [300/938], Loss: 0.2766\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3030\n",
      "Epoch [2/2], Step [500/938], Loss: 0.3306\n",
      "Epoch [2/2], Step [600/938], Loss: 0.3153\n",
      "Epoch [2/2], Step [700/938], Loss: 0.3216\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3317\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3069\n",
      "LR: 0.01, Momentum: 0.9, Accuracy: 0.9178\n",
      "Epoch [1/2], Step [100/938], Loss: 0.6307\n",
      "Epoch [1/2], Step [200/938], Loss: 0.4025\n",
      "Epoch [1/2], Step [300/938], Loss: 0.4400\n",
      "Epoch [1/2], Step [400/938], Loss: 0.3903\n",
      "Epoch [1/2], Step [500/938], Loss: 0.3629\n",
      "Epoch [1/2], Step [600/938], Loss: 0.3725\n",
      "Epoch [1/2], Step [700/938], Loss: 0.3466\n",
      "Epoch [1/2], Step [800/938], Loss: 0.3591\n",
      "Epoch [1/2], Step [900/938], Loss: 0.3621\n",
      "Epoch [2/2], Step [100/938], Loss: 0.3192\n",
      "Epoch [2/2], Step [200/938], Loss: 0.3480\n",
      "Epoch [2/2], Step [300/938], Loss: 0.3700\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3272\n",
      "Epoch [2/2], Step [500/938], Loss: 0.3372\n",
      "Epoch [2/2], Step [600/938], Loss: 0.3610\n",
      "Epoch [2/2], Step [700/938], Loss: 0.3784\n",
      "Epoch [2/2], Step [800/938], Loss: 0.3723\n",
      "Epoch [2/2], Step [900/938], Loss: 0.3644\n",
      "LR: 0.01, Momentum: 0.95, Accuracy: 0.9047\n",
      "Epoch [1/2], Step [100/938], Loss: 0.9306\n",
      "Epoch [1/2], Step [200/938], Loss: 0.7009\n",
      "Epoch [1/2], Step [300/938], Loss: 0.7472\n",
      "Epoch [1/2], Step [400/938], Loss: 0.7288\n",
      "Epoch [1/2], Step [500/938], Loss: 0.7335\n",
      "Epoch [1/2], Step [600/938], Loss: 0.6926\n",
      "Epoch [1/2], Step [700/938], Loss: 0.6391\n",
      "Epoch [1/2], Step [800/938], Loss: 0.6828\n",
      "Epoch [1/2], Step [900/938], Loss: 0.6509\n",
      "Epoch [2/2], Step [100/938], Loss: 0.6354\n",
      "Epoch [2/2], Step [200/938], Loss: 0.6298\n",
      "Epoch [2/2], Step [300/938], Loss: 0.6830\n",
      "Epoch [2/2], Step [400/938], Loss: 0.6510\n",
      "Epoch [2/2], Step [500/938], Loss: 0.6998\n",
      "Epoch [2/2], Step [600/938], Loss: 0.6628\n",
      "Epoch [2/2], Step [700/938], Loss: 0.5098\n",
      "Epoch [2/2], Step [800/938], Loss: 0.6649\n",
      "Epoch [2/2], Step [900/938], Loss: 0.6773\n",
      "LR: 0.1, Momentum: 0.8, Accuracy: 0.8874\n",
      "Epoch [1/2], Step [100/938], Loss: 1.3270\n",
      "Epoch [1/2], Step [200/938], Loss: 1.3991\n",
      "Epoch [1/2], Step [300/938], Loss: 1.4377\n",
      "Epoch [1/2], Step [400/938], Loss: 1.3454\n",
      "Epoch [1/2], Step [500/938], Loss: 1.3732\n",
      "Epoch [1/2], Step [600/938], Loss: 1.3570\n",
      "Epoch [1/2], Step [700/938], Loss: 1.3604\n",
      "Epoch [1/2], Step [800/938], Loss: 1.4535\n",
      "Epoch [1/2], Step [900/938], Loss: 1.2021\n",
      "Epoch [2/2], Step [100/938], Loss: 1.0980\n",
      "Epoch [2/2], Step [200/938], Loss: 1.1021\n",
      "Epoch [2/2], Step [300/938], Loss: 1.2992\n",
      "Epoch [2/2], Step [400/938], Loss: 1.2793\n",
      "Epoch [2/2], Step [500/938], Loss: 1.2004\n",
      "Epoch [2/2], Step [600/938], Loss: 1.2211\n",
      "Epoch [2/2], Step [700/938], Loss: 1.2526\n",
      "Epoch [2/2], Step [800/938], Loss: 1.2006\n",
      "Epoch [2/2], Step [900/938], Loss: 1.3255\n",
      "LR: 0.1, Momentum: 0.9, Accuracy: 0.8939\n",
      "Epoch [1/2], Step [100/938], Loss: 2.2337\n",
      "Epoch [1/2], Step [200/938], Loss: 2.7479\n",
      "Epoch [1/2], Step [300/938], Loss: 2.3785\n",
      "Epoch [1/2], Step [400/938], Loss: 2.6006\n",
      "Epoch [1/2], Step [500/938], Loss: 2.6565\n",
      "Epoch [1/2], Step [600/938], Loss: 2.9499\n",
      "Epoch [1/2], Step [700/938], Loss: 2.8059\n",
      "Epoch [1/2], Step [800/938], Loss: 2.7953\n",
      "Epoch [1/2], Step [900/938], Loss: 2.1334\n",
      "Epoch [2/2], Step [100/938], Loss: 2.2211\n",
      "Epoch [2/2], Step [200/938], Loss: 2.5652\n",
      "Epoch [2/2], Step [300/938], Loss: 2.4575\n",
      "Epoch [2/2], Step [400/938], Loss: 2.6608\n",
      "Epoch [2/2], Step [500/938], Loss: 2.6635\n",
      "Epoch [2/2], Step [600/938], Loss: 2.6031\n",
      "Epoch [2/2], Step [700/938], Loss: 2.7652\n",
      "Epoch [2/2], Step [800/938], Loss: 2.6091\n",
      "Epoch [2/2], Step [900/938], Loss: 3.1961\n",
      "LR: 0.1, Momentum: 0.95, Accuracy: 0.8805\n",
      "\n",
      "Best parameters: {'lr': 0.01, 'momentum': 0.8}\n",
      "Best accuracy: 0.9192\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "momentum_values = [0.8, 0.9, 0.95]\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentum_values:\n",
    "        model = LogisticRegression(input_dim, num_classes)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        \n",
    "        train(model, train_loader, criterion, optimizer, num_epochs=2)\n",
    "        \n",
    "        accuracy, _, _, _ = evaluate(model, test_loader)\n",
    "        print(f\"LR: {lr}, Momentum: {momentum}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'lr': lr, 'momentum': momentum}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_2300\\2718811364.py:3: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_2300\\2718811364.py:4: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_2300\\2718811364.py:5: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = torch.argmax(Z, dim=1).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "mask = (train_dataset.targets < 2)\n",
    "X = train_dataset.data[mask].numpy().reshape(-1, 784)[:, :2]\n",
    "y = train_dataset.targets[mask].numpy()\n",
    "\n",
    "simple_model = LogisticRegression(2, 2)\n",
    "simple_optimizer = optim.SGD(simple_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "plot_decision_boundary(simple_model, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
